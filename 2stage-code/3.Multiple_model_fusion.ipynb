{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用多模型融合\n",
    "\n",
    "1 用训练图像分别导出各finetune好了的模型特征，以及对应label\n",
    "\n",
    "2 构建分类器，并训练，保存权重\n",
    "\n",
    "3 用test图像导出特征，输入分类器预测\n",
    "\n",
    "4 输出预测结果到csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 导出特征和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4750 images belonging to 12 classes.\n",
      "Found 794 images belonging to 1 classes.\n",
      "297/297 [==============================] - 84s 284ms/step\n",
      "50/50 [==============================] - 13s 261ms/step\n",
      "model InceptionV3\n",
      "(4750, 2048)\n",
      "(794, 2048)\n",
      "(4750,)\n",
      "model InceptionV3\n",
      "(4750, 2048)\n",
      "(794, 2048)\n",
      "(4750,)\n",
      "[ 0  0  0 ... 11 11 11]\n",
      "#\n",
      "Found 4750 images belonging to 12 classes.\n",
      "Found 794 images belonging to 1 classes.\n",
      "297/297 [==============================] - 117s 393ms/step\n",
      "50/50 [==============================] - 20s 393ms/step\n",
      "model Xception\n",
      "(4750, 2048)\n",
      "(794, 2048)\n",
      "(4750,)\n",
      "model Xception\n",
      "(4750, 2048)\n",
      "(794, 2048)\n",
      "(4750,)\n",
      "[ 0  0  0 ... 11 11 11]\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "def write_gap(MODEL, image_size, lambda_func=None, weights_file=None, train_imgs_path=None, test_imgs_path=None, model_name=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights=None, include_top=False)\n",
    "    base_model.load_weights(weights_file, by_name=True)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    \n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(train_imgs_path, image_size, shuffle=False, class_mode=\"categorical\",\n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(test_imgs_path, image_size, shuffle=False,\n",
    "                                             batch_size=16, class_mode=None)\n",
    "    \n",
    "    train_img_nums = train_generator.samples\n",
    "    test_img_nums = test_generator.samples\n",
    "    \n",
    "    train = model.predict_generator(train_generator, (train_img_nums//16) + 1, verbose=1)\n",
    "    test = model.predict_generator(test_generator, (test_img_nums//16) + 1, verbose=1)\n",
    "\n",
    "    print(\"model %s\"%(model_name))\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    print((train_generator.classes).shape)\n",
    "    \n",
    "    train = train[:train_img_nums]\n",
    "    test = test[:test_img_nums]\n",
    "    \n",
    "    print(\"model %s\"%(model_name))\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    print((train_generator.classes).shape)\n",
    "    \n",
    "    print(train_generator.classes)\n",
    "    print(\"#\")\n",
    "\n",
    "    with h5py.File(\"gap_%s.h5\"%(model_name)) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "\n",
    "# 输入所有的训练样本，后续在分割\n",
    "#write_gap(ResNet50, (224, 224), \n",
    "#          'ResNet50_finetune.h5', '../dataset/train-ready', '../dataset/test')\n",
    "write_gap(InceptionV3, (299, 299), inception_v3.preprocess_input, \n",
    "          'InceptionV3_finetune.h5', '../dataset/train-ready-all', '../dataset/test/', 'InceptionV3')\n",
    "write_gap(Xception, (299, 299), xception.preprocess_input, \n",
    "          'Xception_finetune.h5', '../dataset/train-ready-all', '../dataset/test', 'Xception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 融合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4750, 2048)\n",
      "(4750, 2048)\n",
      "[5 8 6 ... 1 5 4]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2017)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "# 特征是需要融合的，但label是一致的（没有使用乱序），即多个特征融合后对应同一个label\n",
    "\n",
    "#for filename in [\"gap_Xception.h5\", \"gap_InceptionV3.h5\"]:\n",
    "for filename in [\"gap_Xception.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        y_train = np.array(h['label'])\n",
    "\n",
    "print(np.array(X_train).shape)\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "print(np.array(X_train).shape)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(y_train)\n",
    "#print(\"fusion model\")\n",
    "#print(\"train.shape %d test.shape %d label.shape %d\"%(X_train.shape, X_test.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "y_train = K.one_hot(y_train, 12)\n",
    "y_train = K.eval(y_train)\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 构建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                24588     \n",
      "=================================================================\n",
      "Total params: 24,588\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "x = Dropout(0.5)(input_tensor)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "model = Model(input_tensor, predictions)\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4275 samples, validate on 475 samples\n",
      "Epoch 1/10000\n",
      "4275/4275 [==============================] - 1s 335us/step - loss: 4.1718 - acc: 0.1123 - val_loss: 2.5160 - val_acc: 0.1789\n",
      "Epoch 2/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 3.1474 - acc: 0.1357 - val_loss: 2.4616 - val_acc: 0.1832\n",
      "Epoch 3/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.7376 - acc: 0.1520 - val_loss: 2.3839 - val_acc: 0.1832\n",
      "Epoch 4/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.4673 - acc: 0.1864 - val_loss: 2.3139 - val_acc: 0.1895\n",
      "Epoch 5/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.4062 - acc: 0.2002 - val_loss: 2.1924 - val_acc: 0.2274\n",
      "Epoch 6/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.3094 - acc: 0.2049 - val_loss: 2.3482 - val_acc: 0.2211\n",
      "Epoch 7/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.2747 - acc: 0.2117 - val_loss: 2.2580 - val_acc: 0.1916\n",
      "Epoch 8/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.2391 - acc: 0.2185 - val_loss: 2.2072 - val_acc: 0.2274\n",
      "Epoch 9/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.2345 - acc: 0.2269 - val_loss: 2.1932 - val_acc: 0.2442\n",
      "Epoch 10/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.2092 - acc: 0.2435 - val_loss: 2.3823 - val_acc: 0.2189\n",
      "Epoch 11/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.1961 - acc: 0.2386 - val_loss: 2.0812 - val_acc: 0.2695\n",
      "Epoch 12/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1771 - acc: 0.2412 - val_loss: 2.1225 - val_acc: 0.2295\n",
      "Epoch 13/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1655 - acc: 0.2442 - val_loss: 2.0772 - val_acc: 0.2568\n",
      "Epoch 14/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1465 - acc: 0.2498 - val_loss: 2.0135 - val_acc: 0.3389\n",
      "Epoch 15/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1519 - acc: 0.2519 - val_loss: 2.0855 - val_acc: 0.2547\n",
      "Epoch 16/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1516 - acc: 0.2519 - val_loss: 2.0638 - val_acc: 0.2842\n",
      "Epoch 17/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1382 - acc: 0.2545 - val_loss: 2.1048 - val_acc: 0.2337\n",
      "Epoch 18/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1326 - acc: 0.2585 - val_loss: 2.0543 - val_acc: 0.2989\n",
      "Epoch 19/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1331 - acc: 0.2650 - val_loss: 2.1236 - val_acc: 0.1979\n",
      "Epoch 20/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1293 - acc: 0.2709 - val_loss: 2.1795 - val_acc: 0.1895\n",
      "Epoch 21/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.1204 - acc: 0.2594 - val_loss: 2.1009 - val_acc: 0.2653\n",
      "Epoch 22/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.1582 - acc: 0.2529 - val_loss: 2.0918 - val_acc: 0.2421\n",
      "Epoch 23/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.1287 - acc: 0.2561 - val_loss: 2.0524 - val_acc: 0.2505\n",
      "Epoch 24/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.1203 - acc: 0.2578 - val_loss: 2.0442 - val_acc: 0.2968\n",
      "Epoch 25/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1132 - acc: 0.2671 - val_loss: 1.9822 - val_acc: 0.2800\n",
      "Epoch 26/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0928 - acc: 0.2702 - val_loss: 1.9799 - val_acc: 0.3032\n",
      "Epoch 27/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.1058 - acc: 0.2627 - val_loss: 2.0811 - val_acc: 0.2568\n",
      "Epoch 28/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.1100 - acc: 0.2683 - val_loss: 2.0131 - val_acc: 0.2863\n",
      "Epoch 29/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0914 - acc: 0.2735 - val_loss: 2.0048 - val_acc: 0.2884\n",
      "Epoch 30/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.1086 - acc: 0.2613 - val_loss: 1.9338 - val_acc: 0.3537\n",
      "Epoch 31/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.1037 - acc: 0.2690 - val_loss: 1.9318 - val_acc: 0.2968\n",
      "Epoch 32/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0972 - acc: 0.2695 - val_loss: 2.0182 - val_acc: 0.3053\n",
      "Epoch 33/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0660 - acc: 0.2851 - val_loss: 2.0359 - val_acc: 0.2337\n",
      "Epoch 34/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0984 - acc: 0.2711 - val_loss: 2.0050 - val_acc: 0.3200\n",
      "Epoch 35/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0974 - acc: 0.2749 - val_loss: 1.9425 - val_acc: 0.2884\n",
      "Epoch 36/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0936 - acc: 0.2737 - val_loss: 2.0022 - val_acc: 0.2947\n",
      "Epoch 37/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0955 - acc: 0.2821 - val_loss: 1.9994 - val_acc: 0.3074\n",
      "Epoch 38/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0700 - acc: 0.2896 - val_loss: 1.9721 - val_acc: 0.3347\n",
      "Epoch 39/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0832 - acc: 0.2800 - val_loss: 2.0193 - val_acc: 0.2379\n",
      "Epoch 40/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0782 - acc: 0.2784 - val_loss: 2.0698 - val_acc: 0.2589\n",
      "Epoch 41/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0705 - acc: 0.2851 - val_loss: 1.9781 - val_acc: 0.3284\n",
      "Epoch 42/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0896 - acc: 0.2746 - val_loss: 1.9986 - val_acc: 0.3053\n",
      "Epoch 43/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0879 - acc: 0.2791 - val_loss: 2.1007 - val_acc: 0.2653\n",
      "Epoch 44/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0761 - acc: 0.2856 - val_loss: 1.9785 - val_acc: 0.2926\n",
      "Epoch 45/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0768 - acc: 0.2774 - val_loss: 1.9988 - val_acc: 0.2547\n",
      "Epoch 46/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0477 - acc: 0.2842 - val_loss: 1.9859 - val_acc: 0.2695\n",
      "Epoch 47/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0665 - acc: 0.2770 - val_loss: 1.9405 - val_acc: 0.3053\n",
      "Epoch 48/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0860 - acc: 0.2772 - val_loss: 1.9704 - val_acc: 0.2863\n",
      "Epoch 49/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0864 - acc: 0.2737 - val_loss: 1.9481 - val_acc: 0.3347\n",
      "Epoch 50/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0690 - acc: 0.2887 - val_loss: 1.9629 - val_acc: 0.2716\n",
      "Epoch 51/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0832 - acc: 0.2833 - val_loss: 1.9995 - val_acc: 0.2716\n",
      "Epoch 52/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0620 - acc: 0.2854 - val_loss: 1.9872 - val_acc: 0.2968\n",
      "Epoch 53/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0602 - acc: 0.2788 - val_loss: 1.9546 - val_acc: 0.3621\n",
      "Epoch 54/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0712 - acc: 0.2779 - val_loss: 1.9648 - val_acc: 0.3011\n",
      "Epoch 55/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0440 - acc: 0.2926 - val_loss: 1.9498 - val_acc: 0.2842\n",
      "Epoch 56/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0592 - acc: 0.2711 - val_loss: 1.9295 - val_acc: 0.2779\n",
      "Epoch 57/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0550 - acc: 0.2889 - val_loss: 2.0280 - val_acc: 0.2484\n",
      "Epoch 58/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0693 - acc: 0.2830 - val_loss: 1.8881 - val_acc: 0.3495\n",
      "Epoch 59/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0592 - acc: 0.2774 - val_loss: 1.9310 - val_acc: 0.3326\n",
      "Epoch 60/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0752 - acc: 0.2842 - val_loss: 1.9258 - val_acc: 0.3179\n",
      "Epoch 61/10000\n",
      "4275/4275 [==============================] - 0s 37us/step - loss: 2.0486 - acc: 0.2849 - val_loss: 2.0170 - val_acc: 0.3074\n",
      "Epoch 62/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0743 - acc: 0.2793 - val_loss: 2.0330 - val_acc: 0.2989\n",
      "Epoch 63/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0583 - acc: 0.2802 - val_loss: 1.9531 - val_acc: 0.3074\n",
      "Epoch 64/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0678 - acc: 0.2791 - val_loss: 2.0964 - val_acc: 0.2274\n",
      "Epoch 65/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0590 - acc: 0.2863 - val_loss: 1.9220 - val_acc: 0.2863\n",
      "Epoch 66/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0555 - acc: 0.2833 - val_loss: 1.9439 - val_acc: 0.2863\n",
      "Epoch 67/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0648 - acc: 0.2842 - val_loss: 2.0100 - val_acc: 0.3032\n",
      "Epoch 68/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0508 - acc: 0.2971 - val_loss: 2.0472 - val_acc: 0.2611\n",
      "Epoch 69/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0541 - acc: 0.2830 - val_loss: 1.9555 - val_acc: 0.3011\n",
      "Epoch 70/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0873 - acc: 0.2819 - val_loss: 1.9200 - val_acc: 0.3663\n",
      "Epoch 71/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0787 - acc: 0.2716 - val_loss: 1.8870 - val_acc: 0.3368\n",
      "Epoch 72/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0300 - acc: 0.2996 - val_loss: 1.8658 - val_acc: 0.3747\n",
      "Epoch 73/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0232 - acc: 0.3060 - val_loss: 1.8998 - val_acc: 0.3242\n",
      "Epoch 74/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0502 - acc: 0.2933 - val_loss: 1.9912 - val_acc: 0.2905\n",
      "Epoch 75/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0587 - acc: 0.2819 - val_loss: 1.9550 - val_acc: 0.3432\n",
      "Epoch 76/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0231 - acc: 0.2966 - val_loss: 1.8776 - val_acc: 0.3621\n",
      "Epoch 77/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0539 - acc: 0.2851 - val_loss: 1.9774 - val_acc: 0.3158\n",
      "Epoch 78/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0493 - acc: 0.2933 - val_loss: 1.9569 - val_acc: 0.3200\n",
      "Epoch 79/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0470 - acc: 0.2910 - val_loss: 1.9198 - val_acc: 0.2989\n",
      "Epoch 80/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0556 - acc: 0.2873 - val_loss: 1.9582 - val_acc: 0.3411\n",
      "Epoch 81/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0575 - acc: 0.2875 - val_loss: 1.9276 - val_acc: 0.3411\n",
      "Epoch 82/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0479 - acc: 0.2954 - val_loss: 2.0061 - val_acc: 0.3137\n",
      "Epoch 83/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0446 - acc: 0.2880 - val_loss: 1.9869 - val_acc: 0.3368\n",
      "Epoch 84/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0573 - acc: 0.2912 - val_loss: 1.8796 - val_acc: 0.3979\n",
      "Epoch 85/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0457 - acc: 0.2894 - val_loss: 1.9297 - val_acc: 0.3158\n",
      "Epoch 86/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0408 - acc: 0.2929 - val_loss: 1.9656 - val_acc: 0.2968\n",
      "Epoch 87/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0299 - acc: 0.2947 - val_loss: 1.9471 - val_acc: 0.3432\n",
      "Epoch 88/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0566 - acc: 0.2807 - val_loss: 1.8835 - val_acc: 0.3579\n",
      "Epoch 89/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0350 - acc: 0.2936 - val_loss: 1.9839 - val_acc: 0.3537\n",
      "Epoch 90/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0508 - acc: 0.2847 - val_loss: 1.8645 - val_acc: 0.3621\n",
      "Epoch 91/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0224 - acc: 0.3008 - val_loss: 2.0216 - val_acc: 0.3053\n",
      "Epoch 92/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0521 - acc: 0.2861 - val_loss: 1.8849 - val_acc: 0.3537\n",
      "Epoch 93/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0416 - acc: 0.2975 - val_loss: 1.9030 - val_acc: 0.3474\n",
      "Epoch 94/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0469 - acc: 0.2863 - val_loss: 1.9947 - val_acc: 0.2674\n",
      "Epoch 95/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0298 - acc: 0.2896 - val_loss: 1.8963 - val_acc: 0.3579\n",
      "Epoch 96/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0324 - acc: 0.2950 - val_loss: 1.9169 - val_acc: 0.2926\n",
      "Epoch 97/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0222 - acc: 0.2919 - val_loss: 1.9715 - val_acc: 0.3053\n",
      "Epoch 98/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0394 - acc: 0.2980 - val_loss: 1.9901 - val_acc: 0.2737\n",
      "Epoch 99/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0228 - acc: 0.3015 - val_loss: 1.8527 - val_acc: 0.3347\n",
      "Epoch 100/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0292 - acc: 0.2851 - val_loss: 1.9052 - val_acc: 0.3411\n",
      "Epoch 101/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0440 - acc: 0.2880 - val_loss: 2.0813 - val_acc: 0.3284\n",
      "Epoch 102/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0293 - acc: 0.2961 - val_loss: 1.9065 - val_acc: 0.3116\n",
      "Epoch 103/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0450 - acc: 0.2898 - val_loss: 1.9100 - val_acc: 0.3474\n",
      "Epoch 104/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0457 - acc: 0.2915 - val_loss: 1.9163 - val_acc: 0.2695\n",
      "Epoch 105/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0230 - acc: 0.2898 - val_loss: 1.9023 - val_acc: 0.3411\n",
      "Epoch 106/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0271 - acc: 0.2992 - val_loss: 1.9632 - val_acc: 0.2884\n",
      "Epoch 107/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0232 - acc: 0.3041 - val_loss: 1.9268 - val_acc: 0.3411\n",
      "Epoch 108/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0622 - acc: 0.2922 - val_loss: 2.0026 - val_acc: 0.2758\n",
      "Epoch 109/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0428 - acc: 0.2929 - val_loss: 1.8892 - val_acc: 0.3705\n",
      "Epoch 110/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0452 - acc: 0.2777 - val_loss: 1.9455 - val_acc: 0.3221\n",
      "Epoch 111/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0258 - acc: 0.2898 - val_loss: 1.9379 - val_acc: 0.3284\n",
      "Epoch 112/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0235 - acc: 0.2870 - val_loss: 1.9411 - val_acc: 0.2989\n",
      "Epoch 113/10000\n",
      "4275/4275 [==============================] - 0s 40us/step - loss: 2.0494 - acc: 0.2933 - val_loss: 1.9117 - val_acc: 0.3516\n",
      "Epoch 114/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0345 - acc: 0.2875 - val_loss: 1.8574 - val_acc: 0.3600\n",
      "Epoch 115/10000\n",
      "4275/4275 [==============================] - 0s 37us/step - loss: 2.0407 - acc: 0.3001 - val_loss: 1.8987 - val_acc: 0.3684\n",
      "Epoch 116/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0181 - acc: 0.2973 - val_loss: 1.9095 - val_acc: 0.3747\n",
      "Epoch 117/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0153 - acc: 0.2961 - val_loss: 1.9065 - val_acc: 0.3579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0436 - acc: 0.2931 - val_loss: 1.9541 - val_acc: 0.3579\n",
      "Epoch 119/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0338 - acc: 0.2917 - val_loss: 1.9670 - val_acc: 0.2800\n",
      "Epoch 120/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0299 - acc: 0.2940 - val_loss: 1.8954 - val_acc: 0.3053\n",
      "Epoch 121/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0290 - acc: 0.2957 - val_loss: 1.8661 - val_acc: 0.3768\n",
      "Epoch 122/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0440 - acc: 0.2973 - val_loss: 2.0347 - val_acc: 0.2863\n",
      "Epoch 123/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0179 - acc: 0.3050 - val_loss: 2.0096 - val_acc: 0.2421\n",
      "Epoch 124/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0164 - acc: 0.2978 - val_loss: 1.8886 - val_acc: 0.3432\n",
      "Epoch 125/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0130 - acc: 0.3069 - val_loss: 1.8527 - val_acc: 0.3053\n",
      "Epoch 126/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0074 - acc: 0.2912 - val_loss: 1.9674 - val_acc: 0.3116\n",
      "Epoch 127/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0256 - acc: 0.2884 - val_loss: 2.0534 - val_acc: 0.2232\n",
      "Epoch 128/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0288 - acc: 0.3004 - val_loss: 1.9089 - val_acc: 0.3074\n",
      "Epoch 129/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0161 - acc: 0.3011 - val_loss: 1.8384 - val_acc: 0.3579\n",
      "Epoch 130/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0248 - acc: 0.2952 - val_loss: 1.9776 - val_acc: 0.3158\n",
      "Epoch 131/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0282 - acc: 0.3048 - val_loss: 1.9676 - val_acc: 0.3432\n",
      "Epoch 132/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0206 - acc: 0.2982 - val_loss: 1.8816 - val_acc: 0.3474\n",
      "Epoch 133/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0189 - acc: 0.3011 - val_loss: 1.8917 - val_acc: 0.3600\n",
      "Epoch 134/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0188 - acc: 0.2987 - val_loss: 1.8676 - val_acc: 0.4105\n",
      "Epoch 135/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0223 - acc: 0.2985 - val_loss: 2.0541 - val_acc: 0.2821\n",
      "Epoch 136/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0117 - acc: 0.3041 - val_loss: 1.9391 - val_acc: 0.3200\n",
      "Epoch 137/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0231 - acc: 0.3004 - val_loss: 1.8312 - val_acc: 0.4274\n",
      "Epoch 138/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0239 - acc: 0.3053 - val_loss: 1.8826 - val_acc: 0.3179\n",
      "Epoch 139/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0117 - acc: 0.3050 - val_loss: 2.0282 - val_acc: 0.2884\n",
      "Epoch 140/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0302 - acc: 0.3001 - val_loss: 1.9781 - val_acc: 0.3221\n",
      "Epoch 141/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0015 - acc: 0.2980 - val_loss: 1.8471 - val_acc: 0.3726\n",
      "Epoch 142/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0420 - acc: 0.2968 - val_loss: 1.9292 - val_acc: 0.3242\n",
      "Epoch 143/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0234 - acc: 0.3069 - val_loss: 1.9651 - val_acc: 0.2800\n",
      "Epoch 144/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0290 - acc: 0.2945 - val_loss: 1.8631 - val_acc: 0.3432\n",
      "Epoch 145/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0392 - acc: 0.2889 - val_loss: 1.9662 - val_acc: 0.2779\n",
      "Epoch 146/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0370 - acc: 0.2926 - val_loss: 2.0485 - val_acc: 0.2632\n",
      "Epoch 147/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0132 - acc: 0.3018 - val_loss: 1.9640 - val_acc: 0.3095\n",
      "Epoch 148/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0001 - acc: 0.3043 - val_loss: 1.8820 - val_acc: 0.3284\n",
      "Epoch 149/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0116 - acc: 0.3043 - val_loss: 1.9177 - val_acc: 0.3137\n",
      "Epoch 150/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0289 - acc: 0.2975 - val_loss: 1.9537 - val_acc: 0.3116\n",
      "Epoch 151/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0328 - acc: 0.2961 - val_loss: 1.8809 - val_acc: 0.3537\n",
      "Epoch 152/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0275 - acc: 0.3032 - val_loss: 1.8950 - val_acc: 0.3284\n",
      "Epoch 153/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0186 - acc: 0.3071 - val_loss: 1.9069 - val_acc: 0.3032\n",
      "Epoch 154/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0187 - acc: 0.3027 - val_loss: 1.8646 - val_acc: 0.3179\n",
      "Epoch 155/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0134 - acc: 0.3081 - val_loss: 1.8625 - val_acc: 0.3242\n",
      "Epoch 156/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0172 - acc: 0.3011 - val_loss: 1.9352 - val_acc: 0.2821\n",
      "Epoch 157/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0029 - acc: 0.3106 - val_loss: 2.0164 - val_acc: 0.3221\n",
      "Epoch 158/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0255 - acc: 0.2968 - val_loss: 1.8577 - val_acc: 0.3368\n",
      "Epoch 159/10000\n",
      "4275/4275 [==============================] - 0s 37us/step - loss: 2.0289 - acc: 0.2989 - val_loss: 1.9342 - val_acc: 0.3621\n",
      "Epoch 160/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0248 - acc: 0.2938 - val_loss: 1.8949 - val_acc: 0.3095\n",
      "Epoch 161/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0153 - acc: 0.3088 - val_loss: 2.0040 - val_acc: 0.2547\n",
      "Epoch 162/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0249 - acc: 0.2985 - val_loss: 1.9754 - val_acc: 0.2905\n",
      "Epoch 163/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0116 - acc: 0.2999 - val_loss: 2.0045 - val_acc: 0.2758\n",
      "Epoch 164/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0093 - acc: 0.3041 - val_loss: 1.9842 - val_acc: 0.3158\n",
      "Epoch 165/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0191 - acc: 0.2999 - val_loss: 1.8712 - val_acc: 0.3011\n",
      "Epoch 166/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0208 - acc: 0.3001 - val_loss: 1.8326 - val_acc: 0.3474\n",
      "Epoch 167/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0344 - acc: 0.3004 - val_loss: 1.8548 - val_acc: 0.3305\n",
      "Epoch 168/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0070 - acc: 0.3013 - val_loss: 1.9884 - val_acc: 0.2674\n",
      "Epoch 169/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0418 - acc: 0.2894 - val_loss: 1.9557 - val_acc: 0.3263\n",
      "Epoch 170/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0193 - acc: 0.2994 - val_loss: 1.8623 - val_acc: 0.3432\n",
      "Epoch 171/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0235 - acc: 0.3102 - val_loss: 1.9274 - val_acc: 0.3326\n",
      "Epoch 172/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0225 - acc: 0.3083 - val_loss: 1.9183 - val_acc: 0.3200\n",
      "Epoch 173/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0262 - acc: 0.2968 - val_loss: 1.9827 - val_acc: 0.3368\n",
      "Epoch 174/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0579 - acc: 0.2943 - val_loss: 1.8629 - val_acc: 0.3558\n",
      "Epoch 175/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0116 - acc: 0.3137 - val_loss: 1.8609 - val_acc: 0.3495\n",
      "Epoch 176/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0017 - acc: 0.3102 - val_loss: 1.8200 - val_acc: 0.3726\n",
      "Epoch 177/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0076 - acc: 0.3069 - val_loss: 1.9898 - val_acc: 0.3074\n",
      "Epoch 178/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9968 - acc: 0.3001 - val_loss: 2.0391 - val_acc: 0.3074\n",
      "Epoch 179/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0160 - acc: 0.3055 - val_loss: 1.9431 - val_acc: 0.2842\n",
      "Epoch 180/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0082 - acc: 0.2989 - val_loss: 1.8204 - val_acc: 0.3347\n",
      "Epoch 181/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0237 - acc: 0.3022 - val_loss: 1.8315 - val_acc: 0.3705\n",
      "Epoch 182/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0244 - acc: 0.3069 - val_loss: 1.9603 - val_acc: 0.3095\n",
      "Epoch 183/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0221 - acc: 0.2947 - val_loss: 1.9600 - val_acc: 0.3495\n",
      "Epoch 184/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0251 - acc: 0.3018 - val_loss: 1.8276 - val_acc: 0.3537\n",
      "Epoch 185/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0034 - acc: 0.3130 - val_loss: 2.0422 - val_acc: 0.2547\n",
      "Epoch 186/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0124 - acc: 0.3057 - val_loss: 1.8649 - val_acc: 0.3600\n",
      "Epoch 187/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9892 - acc: 0.3088 - val_loss: 1.9663 - val_acc: 0.3053\n",
      "Epoch 188/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0216 - acc: 0.2961 - val_loss: 1.9441 - val_acc: 0.3137\n",
      "Epoch 189/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0065 - acc: 0.3020 - val_loss: 1.9094 - val_acc: 0.3179\n",
      "Epoch 190/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0311 - acc: 0.2952 - val_loss: 2.0793 - val_acc: 0.3137\n",
      "Epoch 191/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0203 - acc: 0.3025 - val_loss: 1.8800 - val_acc: 0.3789\n",
      "Epoch 192/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0047 - acc: 0.3060 - val_loss: 1.8977 - val_acc: 0.2947\n",
      "Epoch 193/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0184 - acc: 0.2940 - val_loss: 2.0183 - val_acc: 0.2379\n",
      "Epoch 194/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0095 - acc: 0.3057 - val_loss: 1.9568 - val_acc: 0.2842\n",
      "Epoch 195/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0163 - acc: 0.2994 - val_loss: 1.9108 - val_acc: 0.3242\n",
      "Epoch 196/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 1.9892 - acc: 0.3174 - val_loss: 1.8422 - val_acc: 0.3284\n",
      "Epoch 197/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0042 - acc: 0.3142 - val_loss: 1.8790 - val_acc: 0.3411\n",
      "Epoch 198/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0354 - acc: 0.2987 - val_loss: 1.9750 - val_acc: 0.3221\n",
      "Epoch 199/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0344 - acc: 0.3022 - val_loss: 1.8015 - val_acc: 0.4105\n",
      "Epoch 200/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0078 - acc: 0.3055 - val_loss: 1.8968 - val_acc: 0.3221\n",
      "Epoch 201/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0078 - acc: 0.2987 - val_loss: 1.8656 - val_acc: 0.3558\n",
      "Epoch 202/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0266 - acc: 0.2964 - val_loss: 1.8189 - val_acc: 0.3937\n",
      "Epoch 203/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 1.9978 - acc: 0.3179 - val_loss: 1.9009 - val_acc: 0.2968\n",
      "Epoch 204/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0192 - acc: 0.3043 - val_loss: 1.8732 - val_acc: 0.3453\n",
      "Epoch 205/10000\n",
      "4275/4275 [==============================] - 0s 37us/step - loss: 2.0095 - acc: 0.3092 - val_loss: 2.0289 - val_acc: 0.2611\n",
      "Epoch 206/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 1.9957 - acc: 0.3130 - val_loss: 1.8672 - val_acc: 0.3221\n",
      "Epoch 207/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9978 - acc: 0.3130 - val_loss: 1.9124 - val_acc: 0.3874\n",
      "Epoch 208/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0167 - acc: 0.3008 - val_loss: 1.8595 - val_acc: 0.3853\n",
      "Epoch 209/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0058 - acc: 0.3039 - val_loss: 1.9382 - val_acc: 0.2989\n",
      "Epoch 210/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0182 - acc: 0.3106 - val_loss: 1.8835 - val_acc: 0.3432\n",
      "Epoch 211/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9925 - acc: 0.3055 - val_loss: 1.8652 - val_acc: 0.3874\n",
      "Epoch 212/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9798 - acc: 0.3095 - val_loss: 1.8427 - val_acc: 0.3663\n",
      "Epoch 213/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0125 - acc: 0.3069 - val_loss: 1.8817 - val_acc: 0.3453\n",
      "Epoch 214/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0105 - acc: 0.3057 - val_loss: 1.9221 - val_acc: 0.3095\n",
      "Epoch 215/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0272 - acc: 0.2964 - val_loss: 1.8466 - val_acc: 0.4063\n",
      "Epoch 216/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9813 - acc: 0.3200 - val_loss: 2.0492 - val_acc: 0.2632\n",
      "Epoch 217/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0371 - acc: 0.2996 - val_loss: 1.9597 - val_acc: 0.3347\n",
      "Epoch 218/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0182 - acc: 0.3029 - val_loss: 1.8908 - val_acc: 0.3389\n",
      "Epoch 219/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9974 - acc: 0.3041 - val_loss: 1.8572 - val_acc: 0.3663\n",
      "Epoch 220/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0110 - acc: 0.2982 - val_loss: 1.8449 - val_acc: 0.3411\n",
      "Epoch 221/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0097 - acc: 0.3029 - val_loss: 1.9324 - val_acc: 0.3389\n",
      "Epoch 222/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0106 - acc: 0.3006 - val_loss: 1.8859 - val_acc: 0.3432\n",
      "Epoch 223/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9991 - acc: 0.3120 - val_loss: 1.8855 - val_acc: 0.3789\n",
      "Epoch 224/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9852 - acc: 0.3135 - val_loss: 1.9717 - val_acc: 0.2821\n",
      "Epoch 225/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0073 - acc: 0.3113 - val_loss: 2.1123 - val_acc: 0.2400\n",
      "Epoch 226/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0325 - acc: 0.3011 - val_loss: 2.0174 - val_acc: 0.2842\n",
      "Epoch 227/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9947 - acc: 0.3111 - val_loss: 1.8584 - val_acc: 0.3726\n",
      "Epoch 228/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0148 - acc: 0.3099 - val_loss: 1.8919 - val_acc: 0.3158\n",
      "Epoch 229/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0179 - acc: 0.2987 - val_loss: 1.9092 - val_acc: 0.3179\n",
      "Epoch 230/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9951 - acc: 0.3160 - val_loss: 1.9073 - val_acc: 0.3200\n",
      "Epoch 231/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 1.9888 - acc: 0.3139 - val_loss: 1.8737 - val_acc: 0.3158\n",
      "Epoch 232/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0198 - acc: 0.3083 - val_loss: 1.9576 - val_acc: 0.3032\n",
      "Epoch 233/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0268 - acc: 0.2950 - val_loss: 1.8574 - val_acc: 0.3663\n",
      "Epoch 234/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0071 - acc: 0.3057 - val_loss: 1.9093 - val_acc: 0.3263\n",
      "Epoch 235/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0035 - acc: 0.3043 - val_loss: 1.9441 - val_acc: 0.3516\n",
      "Epoch 236/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0104 - acc: 0.3064 - val_loss: 1.9464 - val_acc: 0.2737\n",
      "Epoch 237/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0302 - acc: 0.2908 - val_loss: 1.8949 - val_acc: 0.3263\n",
      "Epoch 238/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0208 - acc: 0.3013 - val_loss: 1.8949 - val_acc: 0.3453\n",
      "Epoch 239/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0178 - acc: 0.2943 - val_loss: 1.9413 - val_acc: 0.3032\n",
      "Epoch 240/10000\n",
      "4275/4275 [==============================] - 0s 39us/step - loss: 2.0069 - acc: 0.2950 - val_loss: 1.9616 - val_acc: 0.2779\n",
      "Epoch 241/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0141 - acc: 0.3029 - val_loss: 1.8669 - val_acc: 0.3326\n",
      "Epoch 242/10000\n",
      "4275/4275 [==============================] - 0s 39us/step - loss: 1.9950 - acc: 0.3113 - val_loss: 1.9074 - val_acc: 0.3074\n",
      "Epoch 243/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9935 - acc: 0.3074 - val_loss: 1.8356 - val_acc: 0.3411\n",
      "Epoch 244/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0298 - acc: 0.3015 - val_loss: 1.8186 - val_acc: 0.3495\n",
      "Epoch 245/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9998 - acc: 0.3088 - val_loss: 1.9989 - val_acc: 0.2863\n",
      "Epoch 246/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 1.9980 - acc: 0.3118 - val_loss: 1.8952 - val_acc: 0.3684\n",
      "Epoch 247/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0139 - acc: 0.3074 - val_loss: 1.8952 - val_acc: 0.3242\n",
      "Epoch 248/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0039 - acc: 0.3013 - val_loss: 1.8424 - val_acc: 0.3453\n",
      "Epoch 249/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9986 - acc: 0.2994 - val_loss: 1.8733 - val_acc: 0.3242\n",
      "Epoch 250/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0040 - acc: 0.3106 - val_loss: 1.9400 - val_acc: 0.3368\n",
      "Epoch 251/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0117 - acc: 0.3020 - val_loss: 1.8600 - val_acc: 0.4211\n",
      "Epoch 252/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0039 - acc: 0.3116 - val_loss: 2.0360 - val_acc: 0.2695\n",
      "Epoch 253/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0090 - acc: 0.3027 - val_loss: 1.8633 - val_acc: 0.3326\n",
      "Epoch 254/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 1.9988 - acc: 0.3071 - val_loss: 1.8619 - val_acc: 0.3200\n",
      "Epoch 255/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0167 - acc: 0.3025 - val_loss: 1.8256 - val_acc: 0.3474\n",
      "Epoch 256/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0223 - acc: 0.3078 - val_loss: 1.9447 - val_acc: 0.2779\n",
      "Epoch 257/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9879 - acc: 0.3046 - val_loss: 1.9472 - val_acc: 0.3263\n",
      "Epoch 258/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0153 - acc: 0.3036 - val_loss: 2.0856 - val_acc: 0.2821\n",
      "Epoch 259/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0145 - acc: 0.3118 - val_loss: 1.8361 - val_acc: 0.3916\n",
      "Epoch 260/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9955 - acc: 0.3111 - val_loss: 1.9097 - val_acc: 0.3474\n",
      "Epoch 261/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9672 - acc: 0.3198 - val_loss: 1.8824 - val_acc: 0.3495\n",
      "Epoch 262/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0025 - acc: 0.3032 - val_loss: 2.0139 - val_acc: 0.2632\n",
      "Epoch 263/10000\n",
      "4275/4275 [==============================] - 0s 39us/step - loss: 2.0170 - acc: 0.2994 - val_loss: 1.8331 - val_acc: 0.3579\n",
      "Epoch 264/10000\n",
      "4275/4275 [==============================] - 0s 43us/step - loss: 1.9914 - acc: 0.3036 - val_loss: 1.9966 - val_acc: 0.2926\n",
      "Epoch 265/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0129 - acc: 0.2996 - val_loss: 1.9552 - val_acc: 0.2442\n",
      "Epoch 266/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9831 - acc: 0.3158 - val_loss: 1.9594 - val_acc: 0.3305\n",
      "Epoch 267/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9950 - acc: 0.3071 - val_loss: 1.8731 - val_acc: 0.3179\n",
      "Epoch 268/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0108 - acc: 0.3006 - val_loss: 1.8622 - val_acc: 0.3621\n",
      "Epoch 269/10000\n",
      "4275/4275 [==============================] - 0s 40us/step - loss: 2.0005 - acc: 0.3060 - val_loss: 1.9443 - val_acc: 0.3347\n",
      "Epoch 270/10000\n",
      "4275/4275 [==============================] - 0s 42us/step - loss: 2.0100 - acc: 0.2989 - val_loss: 1.8290 - val_acc: 0.3642\n",
      "Epoch 271/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9740 - acc: 0.3144 - val_loss: 1.8569 - val_acc: 0.3453\n",
      "Epoch 272/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 1.9878 - acc: 0.3116 - val_loss: 1.8702 - val_acc: 0.3389\n",
      "Epoch 273/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 1.9768 - acc: 0.3041 - val_loss: 1.8826 - val_acc: 0.3179\n",
      "Epoch 274/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0195 - acc: 0.3153 - val_loss: 1.8097 - val_acc: 0.3958\n",
      "Epoch 275/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9973 - acc: 0.3102 - val_loss: 1.9501 - val_acc: 0.3874\n",
      "Epoch 276/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0126 - acc: 0.2968 - val_loss: 1.8646 - val_acc: 0.3474\n",
      "Epoch 277/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0036 - acc: 0.3032 - val_loss: 1.8174 - val_acc: 0.3642\n",
      "Epoch 278/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 1.9999 - acc: 0.3050 - val_loss: 1.8388 - val_acc: 0.3389\n",
      "Epoch 279/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0029 - acc: 0.3139 - val_loss: 1.8808 - val_acc: 0.3747\n",
      "Epoch 280/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0156 - acc: 0.2992 - val_loss: 1.9227 - val_acc: 0.3937\n",
      "Epoch 281/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0231 - acc: 0.2922 - val_loss: 1.8556 - val_acc: 0.3916\n",
      "Epoch 282/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9927 - acc: 0.3188 - val_loss: 1.8132 - val_acc: 0.3389\n",
      "Epoch 283/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0072 - acc: 0.3064 - val_loss: 1.8708 - val_acc: 0.3432\n",
      "Epoch 284/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9911 - acc: 0.3092 - val_loss: 1.8446 - val_acc: 0.3411\n",
      "Epoch 285/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0343 - acc: 0.3046 - val_loss: 1.8406 - val_acc: 0.3432\n",
      "Epoch 286/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0069 - acc: 0.2996 - val_loss: 1.8419 - val_acc: 0.3389\n",
      "Epoch 287/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 1.9700 - acc: 0.3216 - val_loss: 1.8058 - val_acc: 0.3684\n",
      "Epoch 288/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0037 - acc: 0.3130 - val_loss: 1.8031 - val_acc: 0.3726\n",
      "Epoch 289/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 1.9908 - acc: 0.2999 - val_loss: 1.9747 - val_acc: 0.2695\n",
      "Epoch 290/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 1.9850 - acc: 0.3149 - val_loss: 1.9223 - val_acc: 0.3158\n",
      "Epoch 291/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9999 - acc: 0.3055 - val_loss: 1.9131 - val_acc: 0.2926\n",
      "Epoch 292/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0110 - acc: 0.3048 - val_loss: 1.8140 - val_acc: 0.3684\n",
      "Epoch 293/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9924 - acc: 0.3067 - val_loss: 1.8312 - val_acc: 0.3874\n",
      "Epoch 294/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0044 - acc: 0.3090 - val_loss: 1.8830 - val_acc: 0.3979\n",
      "Epoch 295/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0021 - acc: 0.3099 - val_loss: 1.9738 - val_acc: 0.2842\n",
      "Epoch 296/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0032 - acc: 0.3004 - val_loss: 1.9331 - val_acc: 0.2800\n",
      "Epoch 297/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0072 - acc: 0.3025 - val_loss: 1.9374 - val_acc: 0.3284\n",
      "Epoch 298/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 2.0183 - acc: 0.3004 - val_loss: 1.9741 - val_acc: 0.2989\n",
      "Epoch 299/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0005 - acc: 0.3036 - val_loss: 1.9437 - val_acc: 0.3158\n",
      "Epoch 300/10000\n",
      "4275/4275 [==============================] - 0s 36us/step - loss: 2.0093 - acc: 0.3088 - val_loss: 1.8372 - val_acc: 0.4084\n",
      "Epoch 301/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9927 - acc: 0.3041 - val_loss: 1.8791 - val_acc: 0.3389\n",
      "Epoch 302/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9875 - acc: 0.3125 - val_loss: 1.8166 - val_acc: 0.4000\n",
      "Epoch 303/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0082 - acc: 0.3181 - val_loss: 1.8661 - val_acc: 0.3789\n",
      "Epoch 304/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 2.0007 - acc: 0.3102 - val_loss: 1.9096 - val_acc: 0.3284\n",
      "Epoch 305/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 2.0056 - acc: 0.3156 - val_loss: 1.8543 - val_acc: 0.3537\n",
      "Epoch 306/10000\n",
      "4275/4275 [==============================] - 0s 32us/step - loss: 1.9984 - acc: 0.3085 - val_loss: 1.9548 - val_acc: 0.3095\n",
      "Epoch 307/10000\n",
      "4275/4275 [==============================] - 0s 31us/step - loss: 2.0261 - acc: 0.2952 - val_loss: 1.9280 - val_acc: 0.3558\n",
      "Epoch 308/10000\n",
      "4275/4275 [==============================] - 0s 35us/step - loss: 2.0114 - acc: 0.3116 - val_loss: 1.9114 - val_acc: 0.3411\n",
      "Epoch 309/10000\n",
      "4275/4275 [==============================] - 0s 34us/step - loss: 1.9943 - acc: 0.3090 - val_loss: 1.8733 - val_acc: 0.3284\n",
      "Epoch 310/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9849 - acc: 0.3181 - val_loss: 1.8778 - val_acc: 0.3158\n",
      "Epoch 311/10000\n",
      "4275/4275 [==============================] - 0s 33us/step - loss: 1.9810 - acc: 0.3085 - val_loss: 1.8916 - val_acc: 0.3474\n",
      "Epoch 312/10000\n",
      " 128/4275 [..............................] - ETA: 0s - loss: 2.1224 - acc: 0.2969"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5fe4d1fa3024>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10000\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 写入CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"test2\", (224, 224), shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
